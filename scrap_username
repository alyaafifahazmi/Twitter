{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import webbrowser\n",
    "import time\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get API key and API token from your developer.twitter.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"<<<API key>>>\"\n",
    "consumer_secret = \"<<< API token>>>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_uri = 'oob' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timeline_as_df(timeline_list):\n",
    "    columns = set()\n",
    "    allowed_types = [str, int]\n",
    "    tweets_data = []\n",
    "    for status in timeline_list:\n",
    "        status_dict = dict(vars(status))\n",
    "        keys = status_dict.keys()\n",
    "        single_tweet_data = {\"user\": status.user.screen_name, \"author\": status.author.screen_name}\n",
    "        for k in keys:\n",
    "            try:\n",
    "                v_type = type(status_dict[k])\n",
    "            except:\n",
    "                v_type = None\n",
    "            if v_type != None:\n",
    "                if v_type in allowed_types:\n",
    "                    single_tweet_data[k] = status_dict[k]\n",
    "                    columns.add(k)\n",
    "        tweets_data.append(single_tweet_data)\n",
    "\n",
    "\n",
    "    header_cols = list(columns)\n",
    "    header_cols.append(\"user\")\n",
    "    header_cols.append('author')\n",
    "    data = pd.DataFrame(tweets_data, columns=header_cols)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = api.get_user(\"<<<tweet handle>>>\")\n",
    "\n",
    "user_timeline = user.timeline(count = 500, lang = 'en', tweet_mode = \"extended\")\n",
    "\n",
    "data = extract_timeline_as_df(user_timeline).sort_values(by = 'retweet_count', ascending = False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data['full_text']\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(text):   \n",
    "    \n",
    "    text = re.sub('(@)\\w+', '', text)              # Removing @mentions\n",
    "    text = re.sub(r'RT\\S+', '', text)              # Removing RT \n",
    "    text = re.sub('https?://\\S+', '', text)        # Removing hyperlink\n",
    "    text = text.lower()                            # Lowercase everything\n",
    "    \n",
    "    return text\n",
    "\n",
    "tweets = tweets.apply(remove_features)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits all the sentences up which makes it easier to work with\n",
    "\n",
    "all_sentences = []\n",
    "\n",
    "for word in tweets:\n",
    "   all_sentences.append(word) \n",
    "\n",
    "all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = list()\n",
    "\n",
    "for line in all_sentences:\n",
    "    words = line.split()\n",
    "    for w in words:\n",
    "        lines.append(w)\n",
    "        \n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines2 = []\n",
    "\n",
    "for word in lines:\n",
    "    if word != '':\n",
    "        lines2.append(word)\n",
    "        \n",
    "lines2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is stemming the words to their root\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#using lemmatizer instead of stemmer as many last letter 'e' at the end of the words are removed\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "stem = []\n",
    "for word in lines2:\n",
    "    stem.append(wnl.lemmatize(word))\n",
    "    \n",
    "stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stem2 = []\n",
    "\n",
    "#for word in stem:\n",
    "#    if word not in nlp.Defaults.stop_words:\n",
    "#        stem2.append(word)\n",
    "\n",
    "for w in stem:\n",
    "    if w not in stop_words:\n",
    "        stem2.append(w)\n",
    "stem2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(stem2)\n",
    "words = words[0].value_counts()\n",
    "\n",
    "words.sort_values(ascending=False)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop words that doesnt make sense from lemmetization\n",
    "\n",
    "#words = words.drop(['u', 'et', '&amp;'])\n",
    "#words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To show the top 20 words being used\n",
    "\n",
    "words = words[:20,]\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(words.values, words.index, alpha=0.8)\n",
    "plt.title('Top Words Overall')\n",
    "plt.ylabel('Word from Tweet', fontsize=12)\n",
    "plt.xlabel('Count of Words', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "str1 = \" \" \n",
    "stem2 = str1.join(lines2)\n",
    "\n",
    "stem2 = nlp(stem2)\n",
    "\n",
    "label = [(X.text, X.label_) for X in stem2.ents]\n",
    "\n",
    "data_word_entity = pd.DataFrame(label, columns = ['Word','Entity'])\n",
    "\n",
    "data_word_entity = data_word_entity.where(data_word_entity['Entity'] == 'PERSON')\n",
    "\n",
    "data_name = data_word_entity['Word'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = data_name[:10,]\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data_name.values, data_name.index, alpha=0.8)\n",
    "plt.title('Top People Mentioned')\n",
    "plt.ylabel('Word from Tweet', fontsize=12)\n",
    "plt.xlabel('Count of Words', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Create a function to get the subjective or opinionated the text is\n",
    "\n",
    "def getSubjectivity(text):\n",
    "   return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Create a function to get the polarity, positive/negative the text is\n",
    "\n",
    "def getPolarity(text):\n",
    "   return  TextBlob(text).sentiment.polarity\n",
    "\n",
    "\n",
    "# Create two new columns 'Subjectivity' & 'Polarity'\n",
    "\n",
    "data['Subjectivity'] = data['full_text'].apply(getSubjectivity)\n",
    "data['Polarity'] = data['full_text'].apply(getPolarity)\n",
    "\n",
    "# Show the new dataframe with columns 'Subjectivity' & 'Polarity'\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Word cloud visualization\n",
    "#allWords = ' '.join([twts for twts in str(tweets)])\n",
    "wordCloud =  WordCloud(width = 500, height = 300, random_state = 21, max_font_size = 110).generate(str(tweets))\n",
    "\n",
    "plt.imshow(wordCloud, interpolation = \"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to compute negative (-1), neutral (0) and positive (+1) analysis\n",
    "\n",
    "def getAnalysis(score):\n",
    "    if score < 0:\n",
    "      return 'Negative'\n",
    "    elif score == 0:\n",
    "      return 'Neutral'\n",
    "    else:\n",
    "      return 'Positive'\n",
    "\n",
    "data['Analysis'] = data['Polarity'].apply(getAnalysis)\n",
    "# Show the dataframe\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Analysis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, data.shape[0]):\n",
    "    plt.scatter(data[\"Polarity\"][i], data[\"Subjectivity\"][i], color = 'Orange')\n",
    "\n",
    "plt.title('Sentiment Analysis')\n",
    "plt.grid()\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Subjectivity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting and visualizing the counts\n",
    "plt.title('Sentiment Analysis')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Counts')\n",
    "data['Analysis'].value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Analysis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
